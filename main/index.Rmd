---
title: 'Machine Learning for Tweet Sentiment Detection with H2O'
author: 'Cillian Berragan'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  bookdown::html_document2:
    number_sections: false
    smooth_scroll: true
    css: theme/style.css
bibliography: 
    - /home/cjber/drive/bib/zbib.bib
    - bib/rbib.bib
biblio-style: "apalike"
link-citations: true
---

```{r, include=FALSE}
cjrmd::default_html_chunk_opts()

if (!require("pacman")) install.packages("pacman")
pkgs <- c(
    "bibtex",
    "h2o",
    "rmarkdown",
    "bookdown",
    "twitteR"
)
pacman::p_load(pkgs, character.only = T)
write.bib(pkgs, "bib/rbib.bib")
```

```{r model_code, echo=FALSE, results='hide'}
source("../scripts/w2v_twitter.R")
```

# Executive Summary

This report presents the results of a Natural Language sentiment analysis exercise using a dataset containing `r format(nrow(train), big.mark=',')` labelled tweets created by [Figure Eight's Data for Everyone platform](https://appen.com/resources/datasets/) and accessed through [Kaggle](https://kaggle.com). There is also an included test dataset with `r format(nrow(test), big.mark=',')` labelled tweets for evaluation. The data is licenced under the [creative commons attribution 4.0. international licence](https://creativecommons.org/licenses/by/4.0/).

The R package `h2o` [@h2o] is primarily used for the construction of models in this report, providing high level abstractions for working with state of the art machine learning algorithms. This work aims to demonstrate the ability to construct a simplistic pipeline for sentiment analysis of Tweets without the computationally expensive modern Natural Language Processing (NLP) techniques.

This report was compiled using R Markdown [@rmarkdown1], using the Bookdown package [@bookdown1], and a [CSS theme](https://github.com/vincentdoerig/latex-css) created to resemble LaTeX.

# Introduction

With the presence of microblogging platforms such as Twitter, bulk information regarding user sentiment regarding products and events is now openly accessible. Sentiment analysis is essentially a classification task with generally two or three categories, _"positive"_, _"negative"_ and sometimes _"neutral"_. In this case, the data being classified is present as natural language in tweets.

Early sentiment analysis methodologies relied on ruled based approaches to categorisation, taking a dictionary of words, labelled with positive (1) or negative sentiment (-1), and simply summing the values for each word in a document to retrieve a sentiment score [@bakshi2016]. However, these techniques ignore the complex interactions between words and their context, meaning words with a particular tag may be incorrect. This can often be seen in rule based sentiment analysis where the word _"miss"_ is incorrectly identified as negative when used as a title [e.g. using @hu2004a].

Rule based methods in the majority of NLP applications have largely been superseded in favour of machine learning which tends to outperform previous methods with the use of some amount labelled data. While very recent developments in NLP now rely on pure black box thinking, with large computationally expensive deep learning models, this report demonstrates the middle ground, and takes a relatively simple model to achieve good results without the reliance on effective, but computationally expensive modern techniques such as the use of pretrained word embeddings [@pennington2014], language models [@devlin2019], or deep neural networks.

The model selected for this task is the Gradient Boosting Machine [GBM @friedman2001], which takes an ensemble of weak decision trees which are sequentially built in parallel to minimise the loss function. GBM models are traditionally used for classification tasks, performing well, and providing interpretable results. The particular implementation used follows the algorithm specified by @friedman2001a:

<hr>

**Algorithm: Gradient Boosting Machine**

<hr>

Initialize $f_{k 0}=0, k=1,2, \ldots, K$

For $m=1$ to $M:$

1. Set $p_{k}(x)=\frac{e^{f_{k}(x)}}{\sum_{l=1}^{K} e^{f_{i}(x)}}, k=1,2, \ldots, K$
2. For $k=1$ to $K$ :

    a) Compute $r_{i k m}=y_{i k}-p_{k}\left(x_{i}\right), i=1,2, \ldots, N$
    b) Fit a regression tree to the targets $r_{i k m}, i=1,2, \ldots, N,$ giving terminal regions $R_{j i m}, j=1,2, \ldots, J_{m}$
    c) Compute $\gamma_{j k m}=\frac{K-1}{K} \frac{\sum_{z_{i} \in R_{j k m}}\left(r_{i k m}\right)}{\sum_{x_{i} \in R_{j k m}}\left|r_{i k m}\right|\left(1-\mid r_{i k m}\right)}, j=1,2, \ldots, J_{m}$
    d) Update $f_{k m}(x)=f_{k, m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j k m} I\left(x \in R_{j k m}\right)$

Output $\hat{f}_{k}(x)=f_{k M}(x), k=1,2, \ldots, K$

---

# Methods

The training data was first cleaned by removing any web addresses, numbers and long repeated characters using regular expressions. The text was then tokenized using simple space delimitation, and converted to all lowercase. Any word fewer than two characters was also removed.

To prepare for use in the first model, Word2Vec [@mikolov2013] vector embeddings were created using the `h2o.word2vec` function, using these embeddings, each tweet was then encoded to a single dimension using the average Word2Vec embeddings (`h2o.transform`). 

A Gradient Boosting Machine [GBM @friedman2001] model was then built using taking these 100 dimension embeddings as input. This model was then evaluated on unseen test data providing precision, recall, and an F Score. After determining that the model produces a suitable result, a further GBM was constructed using early stopping with a lower learning rate. Stochastic sampling of rows and columns to improve generalisation. More trees using "more than enough" as early stopping means the model shouldn't run for a full duration.

With this sentiment analysis model, the sentiment of unlabelled tweets was extracted. The `twitteR` [@twitteR] package was used to interface with the Twitter API and using the search phrases _"apple"_ and _"linux"_, the sentiment for 10,000 tweets for each phrase was determined.

## Data

# Results

The first GBM model gave  AUC `r gbm_base_auc`, second `r gbm_ifl_auc`.

F score of `r f_score`, recall of `r recall`, and precision of `r precision`. 

```{r, results='hide'}
linux <- read.csv("../data/linux.csv") %>%
    as.h2o()
apple <- read.csv("../data/apple.csv") %>%
    as.h2o()

model <- h2o.upload_model(path = gbm_path)
w2v_model <- h2o.upload_model(path = w2v_path)

linux_preds <- .predict(linux$text, w2v_model, model)
linux_preds <- ifelse(linux_preds$predict == "positive", 1, 0)

apple_preds <- .predict(apple$text, w2v_model, model)
apple_preds <- ifelse(apple_preds$predict == "positive", 1, 0)
```

Ratio of positive sentiment to negative for Linux related tweets `r sum(linux_preds) / nrow(linux_preds)`. Ratio for apple: `r sum(apple_preds) / nrow(apple_preds)`.

Chatterplot from [here](https://towardsdatascience.com/rip-wordclouds-long-live-chatterplots-e76a76896098). Replacing the pie chart of text data.

```{r}
library(tidyverse)
library(ggrepel)
library(tidytext)

gg_scattertext <- function(word, preds, count) {
    ggplot() +
        aes(preds, count, label = word) +
        geom_text_repel(
            segment.alpha = 0,
            aes(colour = preds, size = count)
        ) +
        scale_colour_gradient(
            low = "red",
            high = "green",
            trans = "log10",
            guide = guide_colourbar(
                direction = "horizontal",
                title.position = "top"
            )
        ) +
        scale_size_continuous(
            range = c(3, 10),
            guide = FALSE
        ) +
        scale_x_log10() +
        labs(
            y = "Word frequency",
            x = "Total sentiment (log)"
        ) +
        theme_minimal() +
        theme(
            legend.position = c(.01, .99),
            legend.justification = c("left", "top")
        )
}
```

```{r chatterplot_linux}
linux_plot <- linux$text %>%
    h2o.strsplit(r"(\\W)") %>%
    as.data.frame() %>%
    mutate(
        id = linux$id %>% as.data.frame(),
        preds = linux_preds %>% as.data.frame()
    ) %>%
    pivot_longer(-c(id, preds),
        values_drop_na = TRUE,
        values_to = "word"
    ) %>%
    select(c(id, word, preds)) %>%
    group_by(word) %>%
    anti_join(get_stopwords()) %>%
    filter(nchar(word) > 2) %>%
    filter(
        !grepl("RT", word, ignore.case = TRUE),
        !grepl("Linux", word, ignore.case = TRUE),
        !grepl("https", word, ignore.case = TRUE),
        !grepl("co", word, ignore.case = TRUE),
    ) %>%
    # add 1 so no log(0)
    summarise(preds = sum(preds) + 1, count = n()) %>%
    arrange(desc(count)) %>%
    head(100)

gg_scattertext(linux_plot$word, linux_plot$preds, linux_plot$count)
```

```{r chatterplot_apple}
apple_plot <- apple$text %>%
    h2o.strsplit(r"(\\W)") %>%
    as.data.frame() %>%
    mutate(
        id = apple$id %>% as.data.frame(),
        preds = apple_preds %>% as.data.frame()
    ) %>%
    pivot_longer(-c(id, preds),
        values_drop_na = TRUE,
        values_to = "word"
    ) %>%
    select(c(id, word, preds)) %>%
    group_by(word) %>%
    anti_join(get_stopwords()) %>%
    filter(nchar(word) > 2) %>%
    filter(
        !grepl("https", word, ignore.case = TRUE),
        !grepl("Apple", word, ignore.case = TRUE),
        !grepl("Mac", word, ignore.case = TRUE),
        !grepl("Macbook", word, ignore.case = TRUE),
        !grepl("Pro", word, ignore.case = TRUE),
        !grepl("700", word, ignore.case = TRUE)
    ) %>%
    # add one so no log(0)
    summarise(preds = sum(preds) + 1, count = n()) %>%
    arrange(desc(count)) %>%
    head(100)

gg_scattertext(apple_plot$word, apple_plot$preds, apple_plot$count)
```

## Model Results

# Conclusion

## Recommendations

Test.

<hr>
Word count: `r wordcountaddin::word_count(rprojroot::thisfile())`
<hr>

### References

<div id="refs"></div>

## Appendix {-}

### Preprocessing and Modelling

```{r ref.label='model_code', eval = FALSE, echo = FALSE}
```

### Figures

```{r ref.label='chatterplot_linux', eval = FALSE, echo = TRUE}

```

```{r ref.label='chatterplot_apple', eval = FALSE, echo =TRUE}

```
